<!doctype html><html><head><title>AIT DSAI | Research | Multimodal AI</title><meta name=viewport content="width=device-width,initial-scale=1"></head><body><nav class=sidebar-nav><a class=sidebar-nav-item href=/ title>Home</a>
<a class=sidebar-nav-item href=/about title>About</a>
<a class=sidebar-nav-item href=/research title>Research</a>
<a class=sidebar-nav-item href=/projects title>Projects</a>
<a class=sidebar-nav-item href=/team title>Team</a>
<a class=sidebar-nav-item href=/contact title>Contact</a></nav><main><h1 id=multi-modal-artificial-intelligence>Multi-modal Artificial Intelligence</h1><p>The definition of intelligence is diverse, and so is the definition of artificial intelligence. Yet most mainstream Artificial Intelligence (AI) solutions simplify a task to a Machine Learning problem using a single sensory modality such as images or videos. By this, relevant information such as the audio layer of videos or the text of document images is neglected. Here at AIT, we focus on the complex task of harnessing and combining information from multiple modalities to develop higher-level cognitive AI systems. These systems learn from the correlation of e.g. audio and visual events and provide advanced models, e.g. for security-related applications. By leveraging cross-modal correlations, multimodal AI further reduces the need for manual annotations. We use multimodal AI solutions in many of our applications, such as predictive maintenance or public safety.</p><h1 id=reference-projects>Reference Projects</h1><h1 id=contact>Contact</h1></main></body></html>